const fs = require('fs');
const path = require('path');
const sharp = require('sharp');
const ort = require('onnxruntime-node');

// ==================== ì„¤ì • (Python ì½”ë“œì™€ ë™ê¸°í™”) ====================

const MODEL_CONFIGS = {
    "melon": { width: 230, height: 70, modelFile: "model_melon.onnx" },
    "nol": { width: 210, height: 70, modelFile: "model_nol.onnx" }
};

// ì•ŒíŒŒë²³ ëŒ€ë¬¸ì (A-Z) ë§¤í•‘
const ALPHABETS = "ABCDEFGHIJKLMNOPQRSTUVWXYZ";
const IDX_TO_CHAR = {};
for (let i = 0; i < ALPHABETS.length; i++) {
    IDX_TO_CHAR[i] = ALPHABETS[i];
}
const BLANK_LABEL = 26; // 0~25: A~Z, 26: Blank

// ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================

/**
 * ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
 * - ì´ë¯¸ì§€ë¥¼ ì½ì–´ì„œ Grayscale ë³€í™˜
 * - ëª¨ë¸ í¬ê¸°ì— ë§ê²Œ Resize (Linear/Bilinear ë³´ê°„)
 * - 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ì •ê·œí™” (Normalization)
 * - Float32Array í…ì„œ ë°ì´í„°ë¡œ ë³€í™˜ (NCHW í¬ë§·: 1x1xHxW)
 */
async function preprocessImage(imagePath, config) {
    try {
        // 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜
        const { data, info } = await sharp(imagePath)
            .resize(config.width, config.height, { 
                fit: 'fill',       // ë¹„ìœ¨ ë¬´ì‹œí•˜ê³  ê°•ì œ ë¦¬ì‚¬ì´ì¦ˆ (íŒŒì´ì¬ ë¡œì§ê³¼ ë™ì¼)
                kernel: 'linear'   // Pythonì˜ Image.BILINEARì™€ ìœ ì‚¬
            })
            .grayscale()           // 'L' ëª¨ë“œ ë³€í™˜
            .raw()                 // í”½ì…€ ë°ì´í„° ì¶”ì¶œ
            .toBuffer({ resolveWithObject: true });

        // 2. ì •ê·œí™” ë° Tensor ë³€í™˜ (Uint8 -> Float32, 0~255 -> 0.0~1.0)
        const float32Data = new Float32Array(data.length);
        for (let i = 0; i < data.length; i++) {
            float32Data[i] = data[i] / 255.0;
        }

        // 3. ONNX Runtimeìš© Tensor ìƒì„± (Dims: [Batch=1, Channel=1, Height, Width])
        const tensor = new ort.Tensor('float32', float32Data, [1, 1, config.height, config.width]);
        return tensor;

    } catch (e) {
        throw new Error(`ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: ${e.message}`);
    }
}

/**
 * CTC Decoding í•¨ìˆ˜ (Greedy Search)
 * - Logits(Output)ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ
 * - ì¤‘ë³µëœ ë¬¸ì ì œê±° ë° Blank ë¼ë²¨ ì œê±°
 */
function ctcDecode(outputTensor) {
    // outputTensor êµ¬ì¡°: [seq_len, batch_size, num_classes] 
    // íŒŒì´ì¬ ëª¨ë¸ ì¶œë ¥: (Width/Seq, Batch, Class) -> ì˜ˆ: [Widthí¬ê¸°, 1, 27]
    
    const dims = outputTensor.dims; // [seq_len, batch, num_classes]
    const seqLen = dims[0];
    const numClasses = dims[2]; // 27
    const data = outputTensor.data;

    let predictedText = "";
    let prevIndex = -1;

    // ì‹œí€€ìŠ¤(Time Step) ìˆœíšŒ
    for (let t = 0; t < seqLen; t++) {
        // í˜„ì¬ Time Step (t)ì—ì„œì˜ ArgMax ì°¾ê¸°
        let maxVal = -Infinity;
        let maxIdx = -1;
        
        // í˜„ì¬ stepì˜ ì‹œì‘ ì˜¤í”„ì…‹
        const offset = t * numClasses;

        for (let c = 0; c < numClasses; c++) {
            if (data[offset + c] > maxVal) {
                maxVal = data[offset + c];
                maxIdx = c;
            }
        }

        // CTC ë¡œì§: ì´ì „ ë¬¸ìì™€ ë‹¤ë¥´ê³ , Blankê°€ ì•„ë‹ˆë©´ ì¶”ê°€
        if (maxIdx !== prevIndex && maxIdx !== BLANK_LABEL) {
            if (IDX_TO_CHAR[maxIdx]) {
                predictedText += IDX_TO_CHAR[maxIdx];
            }
        }
        prevIndex = maxIdx;
    }

    return predictedText;
}

// ==================== ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜ ====================

async function runInference(imagePath, modelType) {
    console.time("Inference Time");
    
    // 1. ì„¤ì • ë¡œë“œ
    const config = MODEL_CONFIGS[modelType];
    if (!config) throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤: ${modelType}`);

    const modelPath = path.join(__dirname, 'models', config.modelFile);
    if (!fs.existsSync(modelPath)) throw new Error(`ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${modelPath}`);

    try {
        // 2. ì„¸ì…˜ ìƒì„± (ëª¨ë¸ ë¡œë“œ)
        const session = await ort.InferenceSession.create(modelPath);

        // 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        const inputTensor = await preprocessImage(imagePath, config);

        // 4. ì¶”ë¡  ì‹¤í–‰
        // 'input'ì€ ONNX export ì‹œ ì§€ì •í•œ input_namesì™€ ì¼ì¹˜í•´ì•¼ í•¨
        const feeds = { input: inputTensor };
        const results = await session.run(feeds);

        // 5. ê²°ê³¼ ë””ì½”ë”©
        // 'output'ì€ ONNX export ì‹œ ì§€ì •í•œ output_namesì™€ ì¼ì¹˜í•´ì•¼ í•¨
        const outputTensor = results.output;
        const text = ctcDecode(outputTensor);

        console.timeEnd("Inference Time");
        return text;

    } catch (e) {
        console.error("ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e);
        throw e;
    }
}

// ==================== ì‹¤í–‰ ì˜ˆì œ ====================

// ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì´ ë¶€ë¶„ í˜¸ì¶œ
(async () => {
    // melon
    // í…ŒìŠ¤íŠ¸ìš© ì„¤ì • (ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    const melonImg = "melon.png"; 
    const melon = "melon"; // or "nol"

    console.log(`ğŸš€ ì¶”ë¡  ì‹œì‘ (Type: ${melon}, Img: ${melonImg})`);
    
    try {
        const result = await runInference(melonImg, melon);
        console.log("------------------------------------------------");
        console.log(`ğŸ“ ì˜ˆì¸¡ ê²°ê³¼: ${result}`);
        console.log("------------------------------------------------");
    } catch (error) {
        console.error("âŒ ì‹¤íŒ¨:", error.message);
    }    
    
    const nolImg = "nol.png"; 
    const nol = "nol"; // or "nol"

    console.log(`ğŸš€ ì¶”ë¡  ì‹œì‘ (Type: ${nol}, Img: ${nolImg})`);
    
    try {
        const result = await runInference(nolImg, nol);
        console.log("------------------------------------------------");
        console.log(`ğŸ“ ì˜ˆì¸¡ ê²°ê³¼: ${result}`);
        console.log("------------------------------------------------");
    } catch (error) {
        console.error("âŒ ì‹¤íŒ¨:", error.message);
    }
})();

